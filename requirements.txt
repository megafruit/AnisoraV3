# === Core pinned stack for manual FlashAttention (torch 2.4.x) ===
# Install PyTorch with CUDA 12.1 wheels:
#   pip install --index-url https://download.pytorch.org/whl/cu121 -r requirements.txt
#
# Then install your local FlashAttention wheel (do NOT let pip pick another build):
#   pip install /path/to/flash_attn-2.8.3+cu12torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

torch==2.4.0
torchvision==0.19.0
torchaudio==2.4.0

# FlashAttention intentionally NOT listed here (you will install the local wheel)

# Optional, commonly compatible with torch 2.4 wheels; if unused you can remove
xformers==0.0.27.post2

# Keep ABI stable / avoid resolver fights
numpy>=1.23.5,<2

# Core ML libs
diffusers>=0.31.0
transformers==4.57.1
tokenizers>=0.22.1
accelerate>=1.1.1

# Frequently used runtime libs
tqdm>=4.66.5
imageio>=2.36.0
imageio-ffmpeg>=0.5.1
ftfy>=6.3.0
easydict
dashscope
gradio>=5.0.0
xfuser